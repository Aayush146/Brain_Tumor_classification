# -*- coding: utf-8 -*-
"""AMLS_dataset_loader.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/12olkrwBTPgJbCsg3yvqcXVtwTvaYpV7x
"""

import torch 
import torchvision 
import numpy as np 
import os
import pydicom as dicom
import matplotlib.pyplot as plt 
import pandas as pd
import sklearn
import random 
import cv2 as cv


import torchvision.transforms as transforms
from torch.utils.tensorboard import SummaryWriter
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import Dataset
from torch.utils.data import DataLoader, SubsetRandomSampler
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import make_scorer
from sklearn.base import BaseEstimator, ClassifierMixin
from sklearn.metrics import accuracy_score
from sklearn.model_selection import KFold
import itertools
from itertools import product

class Train_data(Dataset):

    def __init__(self, base_path,label_file, modality_1, modality_2, train_size):

      super().__init__()
      self.data = []
      self.target = []
      self.label_csv = pd.read_csv(label_file)
      self.mods_dict = dict([('FLAIR', 0), ('T1w', 1), ('TwCE', 2), ('T2w', 3)]) # dictionary for each of the modalities
      self.batch_size = 25
      self.patient_dict_train = None
      
      # pull in the the data 
      num_patients = os.listdir(base_path)[:50] # formt the patients list , we randomly choose patiets to create the training dataset
      
      pa_list = []
      
      # since the patient names would be in string format, they need to be converted to match the ones in the label file 
      for i in num_patients:
        if i.endswith('0'):
          pa_list.append(0)
        else:
          string_list = [int(j) for j in i if int(j) != 0] # we can loop over string as well
          num_pa = int(''.join(map(str, string_list)))
          pa_list.append(num_pa)
          
      # there is a miss match between the number fo patient files and patient labels, hence we choose patients present in both files
    
      Id = self.label_csv.iloc[:,0]
      Id = Id.to_list()
      similar = [element for element in pa_list if element in Id]

      list_len = len(similar)
      train_size = int(train_size * list_len) 
      random_patients = random.sample(similar, train_size)# choose 70% of the data for training 
      patient_dict_1 = dict(enumerate(set(random_patients))) # create a dictionary to use for making the validation and test datasets. As we could not use test_train split, dictionaries were used to avoid patients to be resampled in the validation and test datasets.
      self.patient_dict = patient_dict_1

      for counter, i in enumerate(random_patients):

        l = int(self.label_csv[self.label_csv['BraTS21ID'] == i]['MGMT_value'])

        for f in num_patients:
          if f.endswith('0'):
            num_pa = 0
          else:
            string_list = [int(j) for j in f if int(j) != 0]
            num_pa = int(''.join(map(str, string_list)))

          if num_pa == i:
            patient = os.path.join(base_path, f)
            modalities = os.listdir(patient)
            if len(modalities) <= 3:
              continue
            else:
              path_1_mod = os.path.join(patient, modalities[self.mods_dict[modality_1]]) # path of modality 1 (T2w)
              path_2_mod = os.path.join(patient, modalities[self.mods_dict[modality_2]]) # path for modality 2 (FLAIR)

              # images list for each modality 
              mod_1 = os.listdir(path_1_mod)
              mod_2 = os.listdir(path_2_mod)

              zipped = zip(mod_1,mod_2)
              for count, (mo1, mo2)  in enumerate(zipped):
                  # only choose the first 25 images, as taking more images created computational constraints
                if count + 1 > self.batch_size:
                  break
                else:
                  mod_1_path = os.path.join(path_1_mod, mo1)
                  mod_2_path = os.path.join(path_2_mod, mo2)
                  self.data.append((mod_1_path, mod_2_path))
                  self.target.append(l)
                
                

    def __len__(self):
      return len(self.data)
    
    def __getitem__(self, idx):
      # get the images 
      (mod_1_path, mod_2_path) = self.data[idx]
      class_name = self.target[idx]

      ds_1 = dicom.dcmread(mod_1_path).pixel_array
      ds_2 = dicom.dcmread(mod_2_path).pixel_array

      ds_1 = cv.resize(ds_1, dsize=(512, 512), interpolation=cv.INTER_CUBIC)
      ds_2 = cv.resize(ds_1, dsize=(512,512), interpolation=cv.INTER_CUBIC)

      ds_1 = torch.from_numpy(ds_1.astype(float)).float()
      ds_2 = torch.from_numpy(ds_2.astype(float)).float()

      ds_1 = ds_1.reshape(1, ds_1.shape[0], ds_1.shape[1])
      ds_2 = ds_2.reshape(1, ds_2.shape[0], ds_1.shape[1])

      trans = transforms.Compose([transforms.Normalize(mean = (0), std = (1))])
      trans_image_1 = trans(ds_1)
      trans_image_2 = trans(ds_1)

      stacked_image = np.vstack((trans_image_1, trans_image_2)) # stack the two modalities together

      # tranform the label
      try:
        one_hot_target = torch.from_numpy(np.array(class_name))
        trans_target = F.one_hot(one_hot_target, 2)
        trans_target = torch.reshape(trans_target, (2,1))

      except Exception as e:
        print(' the image going wrong is {} with a class name of {}'.format(mod_1_path, class_name))

      return stacked_image, trans_target, class_name

    def get_dict(self):
      return self.patient_dict
        

class dataset_2(Dataset):

  def __init__(self, base_path,label_file, modality_1, modality_2, patient_dict, size, test):

    super().__init__()
    self.data = []
    self.target = []
    self.label_csv = pd.read_csv(label_file)
    self.mods_dict = dict([('FLAIR', 0), ('T1w', 1), ('TwCE', 2), ('T2w', 3)])
    self.batch_size = 25
    self.patient_dict = None 
    self.size = size
 
 
    # pull in the the data 
    num_patients = os.listdir(base_path)[:50]
    dict_list = [i for i in patient_dict.values()]
    patients = [x for x in num_patients if x not in dict_list] # 70% of the entire dataset is gone 
    list_len = len(patients)
    val_size  = int(self.size * list_len)
    if test:
      random_patients = random.sample(patients, 1)
    else:
      random_patients = random.sample(patients, val_size)[:20] # randomly sample from the remaining set 
      patient_dict_1 = dict(enumerate(random_patients)) # get a dictionary of the used patients 
      self.patient_dict = patient_dict_1 # update the paitients used 

    for counter, i in enumerate(random_patients):
      if i.endswith('0'):
        l = int(self.label_csv[self.label_csv['BraTS21ID'] == 0]['MGMT_value'])
        l_1 = l
      else:
        string_list = [int(j) for j in i if int(j) != 0]
        num_pa = int(''.join(map(str, string_list)))
        l = self.label_csv[self.label_csv['BraTS21ID'] == num_pa]
        l_1 = int(l['MGMT_value']) 
    
        
      # get the modalities from the patient 
      patient = os.path.join(base_path, i)
      modalities = os.listdir(patient)
      # some files contained less than 3 modlaities
      if len(modalities) <=3:
        continue
      else:
        path_1 = os.path.join(patient, os.listdir(patient)[self.mods_dict[modality_1]])
        path_2 = os.path.join(patient, os.listdir(patient)[self.mods_dict[modality_2]])

        mod_1 = os.listdir(path_1)
        mod_2 = os.listdir(path_2)
        zipped = zip(mod_1,mod_2)

        for count, (mo1, mo2)  in enumerate(zipped):
          # no more than 25 images should be added to the dataset for each patient modality
          if count + 1 > self.batch_size:
            break
          else:
            mod_1_path = os.path.join(path_1, mo1)
            mod_2_path = os.path.join(path_2, mo2)
            self.data.append((mod_1_path, mod_2_path))
            self.target.append(l_1)


  def __len__(self):
    return len(self.data)
  
  def __getitem__(self, idx):

    # transform the image 
  
    (mod_1_path, mod_2_path)= self.data[idx]
    class_name = self.target[idx]

    ds_1 = dicom.dcmread(mod_1_path).pixel_array
    ds_2 = dicom.dcmread(mod_2_path).pixel_array

    ds_1 = cv.resize(ds_1, dsize=(512, 512), interpolation=cv.INTER_CUBIC)
    ds_2 = cv.resize(ds_1, dsize=(512,512), interpolation=cv.INTER_CUBIC)

    ds_1 = torch.from_numpy(ds_1.astype(float)).float()
    ds_2 = torch.from_numpy(ds_2.astype(float)).float()

    ds_1 = ds_1.reshape(1, ds_1.shape[0], ds_1.shape[1])
    ds_2 = ds_2.reshape(1, ds_2.shape[0], ds_1.shape[1])

    trans = transforms.Compose([transforms.Normalize(mean = (0), std = (1))])
    trans_image_1 = trans(ds_1)
    trans_image_2 = trans(ds_1)

    stacked_image = np.vstack((ds_1, ds_2))

    # tranform the label 
    
    one_hot_target = torch.from_numpy(np.array(class_name))
    trans_target = F.one_hot(one_hot_target, 2)
    trans_target = torch.reshape(trans_target, (2,1))

    # except Exception as e:
    #   print(' The image that is wrong is {} with a class name of {}'.format(mod_1_path, class_name))

    return stacked_image, trans_target, mod_1_path

  def get_dict(self):
    return self.patient_dict

# this pytorch estimator was made for gridsearch, however, it did not work as it took too long to run. 
class PytorchEstimator(BaseEstimator,ClassifierMixin):
  def __init__(self, model, learning_rate, num_epochs, batch_size):

    self.model = model
    self.learning_rate = learning_rate
    self.num_epochs = num_epochs
    self.batch_size = batch_size 
    self.optimizer = optim.Adam(self.model.parameters(), lr= self.learning_rate)
    self.loss_fn = nn.BCELoss()
    self.device = torch.device('cuda')


  def fit(self, validation_set):

    dataloader = torch.utils.data.DataLoader(validation_set, batch_size=self.batch_size, shuffle = False)
    self.model.to(self.device)

    for epoch in range(self.num_epochs):
      print('processing epoch number {}'.format(epoch))
      self.model.train()
      train_loss = 0.0
      for batch, (inputs, targets) in enumerate(dataloader):

      
        inputs = inputs.to(self.device)
        targets = targets.to(self.device)


        # Zero the gradients
        self.optimizer.zero_grad()

        # Forward pass
        outputs = self.model(inputs)

        # Compute loss
        loss = self.loss_fn(outputs, targets)

        # Backward pass
        loss.backward()

        # Update weights
        self.optimizer.step()
        # update the traning_loss
        train_loss += loss.item()

      train_loss /= len(dataloader)
      print(f'Epoch {epoch+1}/{self.num_epochs}, Train Loss: {train_loss:.4f}')

    return self

  def predict(self, inputs):
 
    # Make predictions
    y_pred_tensor = self.model(inputs)

    # Convert PyTorch tensor to numpy array
    y_pred = y_pred_tensor.detach().numpy()

    return y_pred

  def score(self, X, y):
  
    # Make predictions
    y_pred = self.predict(X)

    # Compute score using accuracy
    acc = accuracy_score(y, y_pred)

    return acc